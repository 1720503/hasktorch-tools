{-# LANGUAGE DeriveGeneric #-}
{-# LANGUAGE MultiParamTypeClasses #-}
{-# LANGUAGE RecordWildCards #-}

module MainV0 where

--import Prelude hiding (exp)
import Control.Monad (when,foldM)      --base
import Data.List (foldl')              --base
import Text.Printf (printf)            --base
--hasktorch
import Torch.Tensor (TensorLike(..))
--import Torch.TensorOptions (defaultOpts,withDevice)
--import Torch.Device (Device(..),DeviceType(..))
import Torch.Functional (mseLoss)
import Torch.NN         (sample)
--import Torch.Autograd   (IndependentTensor(..))
import Torch.Train      (update)
import Torch.Optim      (GD(..))
--import Torch.Serialize  (save)
import Torch.Layer.Linear (LinearHypParams(..),linearLayer)
import Torch.Util.Chart (drawLearningCurve)

trainingData :: [([Float],Float)]
trainingData = [([1],2),([2],4),([3],6),([1],2),([3],7)]

testData :: [([Float],[Float])]
testData = [([3],[7])]

main :: IO()
main = do
  let iter = 10000::Int
  initialParams <- sample $ LinearHypParams 1 1
  (trainedParams,_,l) <- foldLoop (initialParams,GD,[]) [1..iter] $ \(params,opt,losses) epoc -> 
    foldLoop (params,opt,losses) trainingData $ \(params',opt',losses') (input,output) -> do
      let loss = mseLoss (linearLayer params' $ asTensor input) (asTensor output) 
      when (epoc `mod` 10 == 0) $ do
        putStrLn $ "Epoc: " ++ show epoc ++ " | Losses: " ++ show loss
      (p,o) <- update params' opt' loss 5e-5
      return (p,o,((asValue loss)::Float):losses')
  print trainedParams
  mapM_ (putStr . printf "%2.3f ") $ reverse l
  drawLearningCurve "graph.png" "Learning Curve" [("this time",reverse l)]

-- | syntactic sugar for looping with foldM
foldLoop :: param -> [i] -> (param -> i -> IO param) -> IO param
foldLoop param iters block = foldM block param iters

foldl2 :: [b] -> a -> (a -> b -> a) -> a
foldl2 lst zero fun = foldl' fun zero lst
